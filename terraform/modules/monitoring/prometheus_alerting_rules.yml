groups:
- name: default
  rules:
  # See docs/monitoring.md for discussion of the PromQL expressions in these alerts.
  - alert: workflow_manager_heartbeat
    expr: (time() - max_over_time(workflow_manager_last_success_seconds[3600s])) > 1800
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "Workflow manager {{ $labels.locality }}-{{ $labels.ingestor }}
        not running"
      description: "At least 30 minutes have passed since workflow-manager
        instance {{ $labels.locality }}-{{ $labels.ingestor }} in environment
        ${environment} last ran successfully"
  - alert: facilitator_intake_failure_rate
    expr: (sum without (status, aggregation_id, instance, node) (rate(facilitator_intake_tasks_finished{status="success"}[1h])))
      / (sum without (status, aggregation_id, instance, node) (rate(facilitator_intake_tasks_finished[1h]))) < 0.90
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "High failure rate for intake worker
        {{ $labels.namespace }}-{{ $labels.service }}"
      description: "intake worker instance {{ $labels.namespace }}-
        {{ $labels.service }} in environment ${environment} is failing
        more than 10% of the time in the last hour"
  - alert: facilitator_aggregate_failure_rate
    # aggregations run much less often than intake so use a larger window of time
    expr: (sum without (status, aggregation_id, instance, node) (rate(facilitator_aggregate_tasks_finished{status="success"}[${aggregation_period}])))
      / (sum without (status, aggregation_id, instance, node) (rate(facilitator_aggregate_tasks_finished[${aggregation_period}]))) < 0.95
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "High failure rate for aggregate worker
        {{ $labels.namespace }}-{{ $labels.service }}"
      description: "aggregate worker instance {{ $labels.namespace}}-
        {{ $labels.service }} in environment ${environment} is failing
        more than 5% of the time in the last 8 hours"
  - alert: facilitator_intake_rejection_rate
    expr: (sum without (status, aggregation_id, instance, node) (rate(facilitator_intake_tasks_finished{status="rejected"}[1h])))
      / (sum without (status, aggregation_id, instance, node) (rate(facilitator_intake_tasks_finished[1h]))) > 0.10
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "High rejection rate for intake worker
        {{ $labels.namespace }}-{{ $labels.service }}"
      description: "intake worker instance {{ $labels.namespace }}-
        {{ $labels.service }} in environment ${environment} has
        rejected more than 10% of tasks in the last hour"
  - alert: facilitator_aggregate_rejection_rate
    expr: (sum without (status, aggregation_id, instance, node) (rate(facilitator_aggregate_tasks_finished{status="rejected"}[${aggregation_period}])))
      / (sum without (status, aggregation_id, instance, node) (rate(facilitator_aggregate_tasks_finished[${aggregation_period}]))) > 0.01
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "High rejection rate for aggregate worker
        {{ $labels.namespace }}-{{ $labels.service }}"
      description: "aggregate worker instance {{ $labels.namespace }}-
        {{ $labels.service }} in environment ${environment} has
        rejected more than 1% of tasks in the last 8 hours"
  - alert: intake_task_queue_growth
    expr: min_over_time(stackdriver_pubsub_subscription_pubsub_googleapis_com_subscription_num_undelivered_messages{subscription_id!~".*-dead-letter",subscription_id=~".*-intake"}[30m]) > 0
    for: 3h
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "PubSub subscription {{ $labels.subscription_id }} not emptying"
      description: "PubSub subscription {{ $labels.subscription_id }} in
      environment ${environment} has had undelivered messages for 3 hours"
  - alert: aggregate_task_queue_growth
    expr: min_over_time(stackdriver_pubsub_subscription_pubsub_googleapis_com_subscription_num_undelivered_messages{subscription_id!~".*-dead-letter",subscription_id=~".*-aggregate"}[30m]) > 0
    for: 10h
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "PubSub subscription {{ $labels.subscription_id }} not emptying"
      description: "PubSub subscription {{ $labels.subscription_id }} in
      environment ${environment} has had undelivered messages for 10 hours"
  - alert: dead_letter_queue
    expr: min_over_time(stackdriver_pubsub_subscription_pubsub_googleapis_com_subscription_num_undelivered_messages{subscription_id=~".*-dead-letter"}[30m]) > 0
    for: 5m
    labels:
      severity: page
    annotations:
      summary: "Undelivered messages in dead letter queue {{ $labels.subscription_id }}"
      description: "There are undelivered messages in the dead letter queue for
        PubSub subscription {{ $labels.subscription_id }} in environment
        ${environment}."
  - alert: intake_task_queue_growth_sqs
    expr: min_over_time(aws_sqs_approximate_number_of_messages_visible_average{queue_name!~"${environment}-.*-dead-letter",queue_name=~"${environment}-.*-intake"}[30m] offset 20m) > 0
    for: 3h
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "SQS queue {{ $labels.queue_name }} not emptying"
      description: "SQS queue {{ $labels.queue_name }} in environment
        ${environment} has had undelivered messages for 3 hours"
  - alert: aggregate_task_queue_growth_sqs
    expr: min_over_time(aws_sqs_approximate_number_of_messages_visible_average{queue_name!~"${environment}-.*-dead-letter",queue_name=~"${environment}-.*-aggregate"}[30m] offset 20m) > 0
    for: 10h
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "SQS queue {{ $labels.queue_name }} not emptying"
      description: "SQS queue {{ $labels.queue_name }} in environment
        ${environment} has had undelivered messages for 10 hours"
  - alert: dead_letter_queue_sqs
    expr: min_over_time(aws_sqs_approximate_number_of_messages_visible_average{queue_name=~"${environment}-.*-dead-letter"}[30m] offset 20m) > 0
    for: 5m
    labels:
      severity: page
    annotations:
      summary: "Undelivered messages in dead letter queue {{ $labels.queue_name }}"
      description: "There are undelivered messages in the dead letter queue for
        SQS queue {{ $labels.queue_name }} in environment ${environment}."
  - alert: prometheus_disk_utilization
    expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim="prometheus-server-claim"}
      / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim="prometheus-server-claim"} < 0.3
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "prometheus-server low disk space"
      description: "There is less than 30% available disk space on the prometheus-server PersistentVolumeClaim"
  - alert: kubernetes_pod_restarting
    expr: (increase(kube_pod_container_status_restarts_total[15m]) * on (namespace, pod) group_left kube_pod_info) > 2
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "Container is in a restart loop"
      description: "Container {{ $labels.container }} is restarting repeatedly in
        pod {{ $labels.pod }}, namespace {{ $labels.namespace }}, and environment ${environment}."
  - alert: kubernetes_pod_waiting
    expr: min_over_time(kube_pod_container_status_waiting[15m]) * kube_pod_container_status_waiting * (kube_pod_container_status_waiting offset 15m) > 0
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "Container is stuck waiting"
      description: "Container {{ $labels.container }} has been waiting for fifteen minutes, in pod
        {{ $labels.pod }}, namespace {{ $labels.namespace }}, and environment ${environment}."
  - alert: kubernetes_pod_pending
    expr: kube_pod_status_phase{phase="Pending"} * (kube_pod_status_phase{phase="Pending"} offset 15m) > 0
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "Pod is stuck pending scheduling"
      description: "Pod {{ $labels.pod }} has been pending for fifteen minutes, in namespace
        {{ $labels.namespace }} and environment ${environment}."
  - alert: ingest_rate_low
    expr: sum by (service, namespace) (rate(facilitator_intake_ingestion_packets_processed[20h])) < 0.25 *
      sum by (service, namespace) (rate(facilitator_intake_ingestion_packets_processed[20h] offset 1d))
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "The rate of intake processing has dropped off steeply compared to yesterday."
      description: "The number of packets from {{ $labels.service }} in locality
        {{ $labels.namespace }} and environment ${environment} is low relative to past levels."
