groups:
- name: default
  rules:
  - alert: workflow_manager_heartbeat
    expr: (time() - workflow_manager_last_success_seconds) > 1800
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "Workflow manager {{ $labels.locality }}-{{ $labels.ingestor }}
        not running"
      description: "At least 30 minutes have passed since workflow-manager
        instance {{ $labels.locality }}-{{ $labels.ingestor }} in environment
        ${environment} last ran successfully"
  - alert: facilitator_intake_failure_rate
    expr: (sum without (status) (rate(facilitator_intake_tasks_finished{status="success"}[1h])))
      / (sum without (status) (rate(facilitator_intake_tasks_finished[1h]))) < 0.95
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "High failure rate for intake worker
        {{ $labels.kubernetes_namespace }}-{{ $labels.kubernetes_name }}"
      description: "intake worker instance {{ $labels.kubernetes_namespace }}-
        {{ $labels.kubernetes_name }} in environment ${environment} is failing
        more than 5% of the time in the last hour"
  - alert: facilitator_aggregate_failure_rate
    # aggregations run much less often than intake so use a larger window of time
    expr: (sum without (status) (rate(facilitator_aggregate_tasks_finished{status="success"}[${aggregation_period}])))
      / (sum without (status) (rate(facilitator_aggregate_tasks_finished[${aggregation_period}]))) < 0.95
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "High failure rate for aggregate worker
        {{ $labels.kubernetes_namespace }}-{{ $labels.kubernetes_name }}"
      description: "aggregate worker instance {{ $labels.kubernetes_namespace}}-
        {{ $labels.kubernetes_name }} in environment ${environment} is failing
        more than 5% of the time in the last 8 hours"
  - alert: facilitator_intake_rejection_rate
    expr: (sum without (status) (rate(facilitator_intake_tasks_finished{status="rejected"}[1h])))
      / (sum without (status) (rate(facilitator_intake_tasks_finished[1h]))) > 0.01
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "High rejection rate for intake worker
        {{ $labels.kubernetes_namespace }}-{{ $labels.kubernetes_name }}"
      description: "intake worker instance {{ $labels.kubernetes_namespace }}-
        {{ $labels.kubernetes_name }} in environment ${environment} has
        rejected more than 1% of tasks in the last hour"
  - alert: facilitator_aggregate_rejection_rate
    expr: (sum without (status) (rate(facilitator_aggregate_tasks_finished{status="rejected"}[${aggregation_period}])))
      / (sum without (status) (rate(facilitator_aggregate_tasks_finished[${aggregation_period}]))) > 0.01
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "High rejection rate for aggregate worker
        {{ $labels.kubernetes_namespace }}-{{ $labels.kubernetes_name }}"
      description: "aggregate worker instance {{ $labels.kubernetes_namespace }}-
        {{ $labels.kubernetes_name }} in environment ${environment} has
        rejected more than 1% of tasks in the last 8 hours"
  - alert: intake_task_queue_growth
    expr: min_over_time(stackdriver_pubsub_subscription_pubsub_googleapis_com_subscription_num_undelivered_messages{subscription_id!~".*-dead-letter",subscription_id=~".*-intake"}[30m]) > 0
    for: 3h
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "PubSub subscription {{ $labels.subscription_id }} not emptying"
      description: "PubSub subscription {{ $labels.subscription_id }} in
      environment ${environment} has had undelivered messages for 3 hours"
  - alert: aggregate_task_queue_growth
    expr: min_over_time(stackdriver_pubsub_subscription_pubsub_googleapis_com_subscription_num_undelivered_messages{subscription_id!~".*-dead-letter",subscription_id=~".*-aggregate"}[30m]) > 0
    for: 10h
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "PubSub subscription {{ $labels.subscription_id }} not emptying"
      description: "PubSub subscription {{ $labels.subscription_id }} in
      environment ${environment} has had undelivered messages for 10 hours"
  - alert: dead_letter_queue
    expr: min_over_time(stackdriver_pubsub_subscription_pubsub_googleapis_com_subscription_num_undelivered_messages{subscription_id=~".*-dead-letter"}[30m]) > 0
    for: 5m
    labels:
      severity: page
    annotations:
      summary: "Undelivered messages in dead letter queue {{ $labels.subscription_id }}"
      description: "There are undelivered messages in the dead letter queue for
        PubSub subscription {{ $labels.subscription_id }} in environment
        ${environment}."
  - alert: intake_task_queue_growth_sqs
    expr: min_over_time(aws_sqs_approximate_number_of_messages_visible_average{queue_name!~"${environment}-.*-dead-letter",queue_name=~"${environment}-.*-intake"}[30m] offset 20m) > 0
    for: 3h
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "SQS queue {{ $labels.queue_name }} not emptying"
      description: "SQS queue {{ $labels.queue_name }} in environment
        ${environment} has had undelivered messages for 3 hours"
  - alert: aggregate_task_queue_growth_sqs
    expr: min_over_time(aws_sqs_approximate_number_of_messages_visible_average{queue_name!~"${environment}-.*-dead-letter",queue_name=~"${environment}-.*-aggregate"}[30m] offset 20m) > 0
    for: 10h
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "SQS queue {{ $labels.queue_name }} not emptying"
      description: "SQS queue {{ $labels.queue_name }} in environment
        ${environment} has had undelivered messages for 10 hours"
  - alert: dead_letter_queue_sqs
    expr: min_over_time(aws_sqs_approximate_number_of_messages_visible_average{queue_name=~"${environment}-.*-dead-letter"}[30m] offset 20m) > 0
    for: 5m
    labels:
      severity: page
    annotations:
      summary: "Undelivered messages in dead letter queue {{ $labels.queue_name }}"
      description: "There are undelivered messages in the dead letter queue for
        SQS queue {{ $labels.queue_name }} in environment ${environment}."
  - alert: prometheus_disk_utilization
    expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim="prometheus-server-claim"}
      / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim="prometheus-server-claim"} < 0.3
    for: 5m
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "prometheus-server low disk space"
      description: "There is less than 30% available disk space on the prometheus-server PersistentVolumeClaim"
  - alert: kubernetes_pod_restarting
    expr: increase(kube_pod_container_status_restarts_total[15m]) > 2
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "Container is in a restart loop"
      description: "Container {{ $labels.container }} is restarting repeatedly in
        pod {{ $labels.pod }}, namespace {{ $labels.namespace }}, and environment ${environment}."
  - alert: kubernetes_pod_waiting
    expr: min_over_time(kube_pod_container_status_waiting_reason[15m]) > 0
    labels:
      severity: page
      environment: ${environment}
    annotations:
      summary: "Container is stuck waiting"
      description: "Container {{ $labels.container }} has been waiting for fifteen minutes, in pod
        {{ $labels.pod }}, namespace {{ $labels.namespace }}, and environment ${environment}."
